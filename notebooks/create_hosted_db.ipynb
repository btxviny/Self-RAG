{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pdf and split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "# Load your PDF document\n",
    "pdf_path = \"./DLVS.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(len(documents))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=1000,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)\n",
    "documents = [Document(page_content=x.page_content, metadata=x.metadata, id = idx) for idx,x in enumerate(documents)]\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [x.id for x in documents]\n",
    "metadata = [x.metadata for x in documents]\n",
    "docs = [x.page_content for x in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RP963AP\\OneDrive - EY\\Desktop\\RAG-Thesis\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collection(id=f5563a7f-2e8d-4b17-a094-641a5fd980be, name=thesis)]\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection_name = 'thesis'\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')\n",
    "print(list(client.list_collections()))\n",
    "#delete collection if it already exists\n",
    "if collection_name in client.list_collections():\n",
    "    client.delete_collection(collection_name)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis_collection = client.create_collection( \n",
    "        collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        embedding_function=ef,\n",
    "        get_or_create=True\n",
    ")\n",
    "thesis_collection.add(documents = docs, ids =ids, metadatas = metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if we can load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RP963AP\\OneDrive - EY\\Desktop\\RAG-Thesis\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')\n",
    "thesis_collection = client.get_collection(\n",
    "    'thesis',\n",
    "    embedding_function = ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"DIFRINT\"\n",
    "matches = thesis_collection.query(\n",
    "    query_texts=query, \n",
    "    n_results= 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['100 \\n 7.1 DIFRINT Implementation  \\nDIFRINT proposes a deep frame interpolation architecture intended for   video \\nstabilization, that is depicted in Figure 7.1. The training will take place in an \\nunsupervised manner. We will go over the training and testing scheme separately as \\nthey differ and finally, we will introduce two inference parameters: number of \\niterations and the ‘skip’ parameter, which are both cruci al for achieving high quality \\nstabilization.  \\n \\nFigure 7.1: The DIFRINT   [40] framework during (a) training and (b) testing.  \\n \\n7.1.1  Training Scheme  \\nThe basic idea is that given two adjacent frames 𝑓𝑖−1,𝑓𝑖+1 we want to generate the \\nintermediate frame 𝑓𝑖𝑛𝑡. The first part is generating the input to the network. Instead \\nof simply concatenating the two images and feeding them to the network, the authors \\nwarp the two frames towards the intermediate frame 𝑓𝑖, through optical flow \\nestimated with PWC -Net [8], thus producing the warped frames 𝑓𝑤−,𝑓𝑤+.  Both these \\nframes represent half -way points that originate from 𝑓𝑖−1,𝑓𝑖+1. The warped frames \\nare then concatenated across the channel dimension and fed into a U -Net module, \\nwhich learns how to combine information at different scales. Each individual input \\nframe may contain unseen regions, but combined they complement each other , \\nenabling the U -Net to learn how to fill those regions. However powerful the U -Net \\nmight be at learning latent space representations at different scales, using its outputs \\nin an iterative manner is prone to introduce blurring and artifacts. This is more \\nprominently apparent at the boundaries of dynamically moving objects. To mitigate \\nthis effect the authors, add a ResNet module in series, with the aim to reintroduce fine \\ndetails into the interpolated output of the U -Net. The input to the ResNet is the \\ninterpolated frame along with the original frame 𝑓𝑖 warped towards 𝑓𝑖𝑛𝑡 which \\ngenerate the final, high -quality frame 𝑓𝑖̂. A key issue that arises when attempting to',\n",
       "  '112 \\n  \\nThe capabilities of the CAIN network can be better understood by reviewing its \\ncomparative results to DIFRINT. It largely outscores DIFRINT in the stability score, \\nwhich is the most important.  The success of CAIN can be attributed to the use of \\nPixelShuffle which allows it to retrain a large receptive field, without any loss of \\ninformation, combined with its ingenious use of channel attention. A visual \\ncomparison of the generated results is shown in Figure 5.12.  \\n \\nFigure 7.11 : Visual comparison of the results of the two models. The results of CAIN \\nshow almost no distortion as opposed to the highlighted regions of DIFRINT’s result.  \\nWhile DIFRINT needs to be run for five iterations, the results obtained with CAIN used \\nonly 3 iterations. This resulted in an inference time of  4 minutes to stabilize a 40 \\nsecond video with spatial dimensions 360 ×640 as opposed to DIFRINT’s 6 minutes.  \\nI provide the training code and test ing scheme for this algorithm in this repository \\nhttps://github.com/btxviny/Video -Stabilization -through -Frame -Interpolation -using -\\nCAIN .  0.750 0.800 0.850 0.900 0.950 1.000 1.050Cropping ScoreDistortion ScoreStability ScorePixel ScoreCAIN vs DIFRINT\\nDIFRINT-VIMEO CAIN',\n",
       "  '107 \\n 7.1.6 Evaluation Results  \\n \\n \\nFrom the comparative evaluations results we see that the instance of DIFRINT \\nfinetuned on the Vimeo -Dataset, heavily outperforms the instance trained on DAVIS \\non all scores except Stability and Cropping. Both models score very high on cropping \\nas they do n ot introduce any black borders. The difference in the stability scores is \\nnegligible and it’s a necessary trade off in order for the Vimeo trained model to \\nproduce clear images. The visual comparison of the results generated by both models \\nafter 5 iteratio ns with the skip parameter set to 2 is shown in Figure 7.8. \\n \\nFigure 7.8: Visual comparison of the results of the two models.  In the highlighted \\nsections of the images the difference in sharpness and clarity is very apparent.   \\n0.750 0.800 0.850 0.900 0.950 1.000 1.050Cropping ScoreDistortion ScoreStability ScorePixel ScoreAverage Scores Over All Video Categories\\nDIFRINT-VIMEO DIFRINT-DAVIS']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings= HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"thesis\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 110, 'source': './DLVS.pdf'}, page_content='100 \\n 7.1 DIFRINT Implementation  \\nDIFRINT proposes a deep frame interpolation architecture intended for   video \\nstabilization, that is depicted in Figure 7.1. The training will take place in an \\nunsupervised manner. We will go over the training and testing scheme separately as \\nthey differ and finally, we will introduce two inference parameters: number of \\niterations and the ‘skip’ parameter, which are both cruci al for achieving high quality \\nstabilization.  \\n \\nFigure 7.1: The DIFRINT   [40] framework during (a) training and (b) testing.  \\n \\n7.1.1  Training Scheme  \\nThe basic idea is that given two adjacent frames 𝑓𝑖−1,𝑓𝑖+1 we want to generate the \\nintermediate frame 𝑓𝑖𝑛𝑡. The first part is generating the input to the network. Instead \\nof simply concatenating the two images and feeding them to the network, the authors \\nwarp the two frames towards the intermediate frame 𝑓𝑖, through optical flow \\nestimated with PWC -Net [8], thus producing the warped frames 𝑓𝑤−,𝑓𝑤+.  Both these \\nframes represent half -way points that originate from 𝑓𝑖−1,𝑓𝑖+1. The warped frames \\nare then concatenated across the channel dimension and fed into a U -Net module, \\nwhich learns how to combine information at different scales. Each individual input \\nframe may contain unseen regions, but combined they complement each other , \\nenabling the U -Net to learn how to fill those regions. However powerful the U -Net \\nmight be at learning latent space representations at different scales, using its outputs \\nin an iterative manner is prone to introduce blurring and artifacts. This is more \\nprominently apparent at the boundaries of dynamically moving objects. To mitigate \\nthis effect the authors, add a ResNet module in series, with the aim to reintroduce fine \\ndetails into the interpolated output of the U -Net. The input to the ResNet is the \\ninterpolated frame along with the original frame 𝑓𝑖 warped towards 𝑓𝑖𝑛𝑡 which \\ngenerate the final, high -quality frame 𝑓𝑖̂. A key issue that arises when attempting to'),\n",
       " Document(metadata={'page': 122, 'source': './DLVS.pdf'}, page_content='112 \\n  \\nThe capabilities of the CAIN network can be better understood by reviewing its \\ncomparative results to DIFRINT. It largely outscores DIFRINT in the stability score, \\nwhich is the most important.  The success of CAIN can be attributed to the use of \\nPixelShuffle which allows it to retrain a large receptive field, without any loss of \\ninformation, combined with its ingenious use of channel attention. A visual \\ncomparison of the generated results is shown in Figure 5.12.  \\n \\nFigure 7.11 : Visual comparison of the results of the two models. The results of CAIN \\nshow almost no distortion as opposed to the highlighted regions of DIFRINT’s result.  \\nWhile DIFRINT needs to be run for five iterations, the results obtained with CAIN used \\nonly 3 iterations. This resulted in an inference time of  4 minutes to stabilize a 40 \\nsecond video with spatial dimensions 360 ×640 as opposed to DIFRINT’s 6 minutes.  \\nI provide the training code and test ing scheme for this algorithm in this repository \\nhttps://github.com/btxviny/Video -Stabilization -through -Frame -Interpolation -using -\\nCAIN .  0.750 0.800 0.850 0.900 0.950 1.000 1.050Cropping ScoreDistortion ScoreStability ScorePixel ScoreCAIN vs DIFRINT\\nDIFRINT-VIMEO CAIN'),\n",
       " Document(metadata={'page': 117, 'source': './DLVS.pdf'}, page_content='107 \\n 7.1.6 Evaluation Results  \\n \\n \\nFrom the comparative evaluations results we see that the instance of DIFRINT \\nfinetuned on the Vimeo -Dataset, heavily outperforms the instance trained on DAVIS \\non all scores except Stability and Cropping. Both models score very high on cropping \\nas they do n ot introduce any black borders. The difference in the stability scores is \\nnegligible and it’s a necessary trade off in order for the Vimeo trained model to \\nproduce clear images. The visual comparison of the results generated by both models \\nafter 5 iteratio ns with the skip parameter set to 2 is shown in Figure 7.8. \\n \\nFigure 7.8: Visual comparison of the results of the two models.  In the highlighted \\nsections of the images the difference in sharpness and clarity is very apparent.   \\n0.750 0.800 0.850 0.900 0.950 1.000 1.050Cropping ScoreDistortion ScoreStability ScorePixel ScoreAverage Scores Over All Video Categories\\nDIFRINT-VIMEO DIFRINT-DAVIS'),\n",
       " Document(metadata={'page': 109, 'source': './DLVS.pdf'}, page_content='99 \\n Chapter 7:  Video Stabilization through Frame \\nInterpolation  \\n \\nIntroduction  \\nAs we briefly introduced in 6.3.1, an unsteady video sequence can be stabilized \\nthrough frame interpolation which essentially operates like a low -pass filter. By \\ngenerating an intermediate/middle frame 𝐼𝑡+0.5̂   between two preexisting consecutive \\nframes 𝐼𝑡,𝐼𝑡+1 in an iterative manner we can alleviate the sequence of jerkiness and \\nsudden motion, often present in amateur captured video. Th rough this process  we \\ncreate the intermediate frame between two adjacent frames. This idea was originally \\nintroduced by [39]. From an interpolation standpoint, the interpolated middle frame \\nserves as the representation of the frame that would have been recorded between \\ntwo consecutive frames. Consequently, the interpolated frame is indicative of the \\ntemporal midpoint, presumed to  align precisely with the halfway point of inter -frame \\nmotion. Through the course of this chapter, we will dive into Deep Learning Frame \\nInterpolation Techniques and how they can be finetuned for video stabilization. We \\nwill discuss my implementation of DIFRINT  [40], a deep neural network trained \\nspecifically for video stabilization. We will then compare it to CAIN  [41], an award -\\nwinning pretrained network for frame interpolation, which was used in 6.3.1  for \\ngenerating a synthetic supervised dataset.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_from_client.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
